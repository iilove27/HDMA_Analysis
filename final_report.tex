%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Fair or Not: Evidence from HMDA Data
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Anji Zhao (az529), Chenghao Li (cl2567), Yiwei Zhang (yz2454)}% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
%\thanks{$^{1}$H. Kwakernaak is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%        University of Twente, 7500 AE Enschede, The Netherlands
%        {\tt\small h.kwakernaak at papercept.net}}%
%\thanks{$^{2}$P. Misra is with the Department of Electrical %Engineering, Wright State University,
%        Dayton, OH 45435, USA
%        {\tt\small p.misra at ieee.org}}%
%}
\usepackage{graphicx} %package to manage images
\graphicspath{ {./images/} }
\usepackage[rightcaption]{sidecap}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{array}
\newcolumntype{L}{>{\arraybackslash}m{4cm}}
\newcolumntype{C}{>{\arraybackslash}m{2cm}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In this paper, we constructed an empirical analysis to apply machine learning techniques to classify mortgage applications using the HMDA dataset. Moreover, we explored whether race plays an important role in the mortgage application process. We believe that the algorithms can not only be used to classify mortgage applications, but can also provide some insights for the regulatory departments to check the fairness of loan-granting procedure. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Housing mortgage loan granting procedure relies on applicants' background information, and it is important for the financial institutions to predict whether an applicant should be approved or not in order to minimize their credit risk exposure. As we all know, in 2008, subprime mortgage default was the direct reason that caused the global financial crisis. Hence, it is meaningful to develop a good model to classify these mortgage applications. However, as machine learning algorithms usually achieve their goals by seeking existing patterns in the data, we do not know whether the algorithms classify the mortgage application "fairly". As mentioned in the Equal Credit Opportunity Act, discrimination in credit applications based on race, color, religion, national origin, sex is unlawful. Different races, ethnicity, genders are supposed to have equal opportunities to get approved. Therefore, it is also important for us to ensure that our models are fairly classifying mortgage applications.

In this project, HMDA data from District of Columbia is used to explore the fairness of loan granting procedure. D.C. has a highly diversified population, which provides less biased dataset for this project. Dataset has all loan application records ranging from 2007 to 2012. Note that 2007 is during the global financial crisis, therefore, data in 2007 may provide some different insights.

\section{EXPLORATORY DATA ANALYSIS}
\subsection{Data Characteristics}
We selected and downloaded the HMDA dataset for District of Columbia from 2007 to 2012. The dataset has 36 feature columns and 254,976 records of loan application. For every loan application record, outcome is nominal with value 1 to 5, with each representing being approved, rejected, etc. The data also contains some nominal value variables including loan agency names, loan types, property types, and most importantly for this project, applicants' race and ethnicity. Other than these categorical features, there are two features with continuous values, which are loan amount and applicants' income. 

The data is complete by itself without any missing values, there are, however, certain features having values of "No information provided" or "Not applicable", these values will be considered missing values in this study. In addition, some features are involved with co-applicants' information, while most of the applications are filed alone, which also makes the dataset messy. We picked 17 most relevant features to conduct this study.

Below is the features being used in this project:
\begin{itemize}

\item Categorical: Year, Agency, Loan Type, Property Type, Loan Purpose, Owner Occupancy, Loan Amount, Preapproval Status, Applicant Ethnicity, Co-applicant Ethnicity, Applicant Race, Co-applicant Race, Applicant Sex, Co-applicant Sex, Purchaser Type
\item Continuous: Applicant income, Loan Amount
\item Output: Action taken

\end{itemize}
\subsection{Data Visualization}
Correlation matrix serves as a great preview tool to see the relationships between features. We used heatmap to visualize the matrix:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{heatmap.png}
    \caption{Correlation matrix visualization between all features}
    \label{fig:mesh2}
\end{figure}

We observe that except (co-)applicant race and ethnicity have relatively strong correlation, correlations are low among almost all other features. Hence, co-linearity is not significant between those features.

Distributions on the features or the output space ($y$) are important as well in order to have a basic understanding of the data. For continuous features like applicant income and loan amount, there are some extreme outliers, which can be seen from their distributions in Figure 2 and Figure 3. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{loan_amount.png}
    \caption{Histogram of loan amount being applied}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{income.png}
    \caption{Histogram of applicants' income level}
\end{figure}

We can observe that most loan amount being applied are around 250,000 dollars, and majority of applicants have annual income around 100,000 dollars. But some loan amount exceeds 1,750,000 dollars while some applicants claimed to have an income level over 800,000 dollars.

For output space, to have a first check of the fairness of the loan granting process, it is ideal to group records by sex or race, and plot the bar charts separately.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{action_sex.png}
    \caption{Comparison between male and female}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{action_race.png}
    \caption{Comparison among races}
\end{figure}


Based on Figure 4, we cannot see much difference in pattern for male applicants and female applicants, their distributions are similar. For distributions of difference races, however, we observe that African American, Pacific Islander and Native American have higher denial rates compared to White and Asian. 

\subsection{Feature Engineering}
We applied the techniques learned in the class and did several feature engineering work. We will discuss each of them in detail below: 
\begin{itemize}
    \item \textbf{Missing Values:} We explored the features and found the variable applicant\_income\_000s contains 7517 missing values. Since this variable indicates the applicant's income level and from the visualization part we know it contains extreme outliers. As a result, we chose to fill the missing values with the median, which is 112.
    \item \textbf{One-hot Encoding:} We created dummy variables for the categorical features. Specifically, as discussed in the lecture, we chose one-hot encoding to transform all the categorical features to binary values by introducing additional dummy variables.
    \item \textbf{Handling Outliers:} We handled outliers in the data set. We know that the two real value variables, applicant income and loan amount contains extreme outliers and the causes of these outliers are most likely to be incorrect information. So we dropped the entries which have applicant's income or loan amount that is three standard deviations above the mean.
    \item \textbf{Merge Classes:} The objective of this project is to study loan approval vs denial. Thus, some actions taken by the financial institutions can be mapped into the two binary classes: Approved or Denied. Also, some classes have very few samples or their definitions are very similar to some other classes, so we merged the classes. Specifically, we mapped the observations of "Preapproval request approved but not accepted", "Application approved but not accepted", "Loan purchased by institutions" into the "Loan originated" class. Also, we added the observations of "File closed for incompleteness" and "Preapproval request denied by financial institution" into the "Application denied by financial institution" class. In this way, we made the output $y$ a binary variable and made the problem a binary classification problem.
    \item \textbf{Rebalancing Classes:} We used Synthetic Minority Oversampling Technique (SMOTE)\cite{c2} methods to oversample the minority classes in order to balance the data set. SMOTE method is based on nearest-neighbor judged by Euclidean distance, and it synthesizes new samples to balance minority class with the dominating class. Balancing the classes is important to prevent algorithms from overfitting on the majority classes.
    \item \textbf{Normalizing the Data:} We normalized the data using MinMaxScaler. We chose MinMaxScaler because it preserves the original data distribution. Moreover, it does not transform one-hot encoded features which might create disastrous result. Mathematically, MinMaxScaler can be formulated as:
    \begin{equation}
        x_i = \frac{x_i-min(x)}{max(x)-min(x)}
    \end{equation}
    
\end{itemize}
After doing all these feature engineering work, we split the data into training set and testing set (0.7:0.3).

\subsection{Overfit/Underfit Problem}
Three methods were used to prevent our models from overfitting/underfitting the data.

\begin{itemize}
\item Minority classes were resampled to balance the dataset such that accuracy score would be sensical, also to make the model learn enough information from the minority classes.
\item Automated 5-fold cross-validation was used to tune parameters, and to prevent overfitting/underfitting by choosing the best sets of parameters.
\item We tested each model by adding different regularizers and applied grid search approaches to search for the best parameters. Regularization in general can prevent algorithms from overfitting effectively.
\end{itemize}


\section{Model Implementation}

\subsection{Logistic Regression}
Logistic Regression is one of the fundamental classification methods that have been discussed in class. It is often used for binary classifications since Sigmoid function gives probabilities of the two classes between 0 and 1. L1 and L2 regularization methods were tried to penalize coefficients and further prevent overfitting. We tested the regularization parameter $\lambda$ in the set of $\{0.01,\ 0.1,\ 1,\ 10\}$. Also, we set the maximum iterations number to be 1000 in order to make the algorithm fully converged.

The objective functions for L1 and L2 regularization methods are:
\begin{equation}
    \mathcal{L}(\omega) = \Sigma_{i=1}^{n}log(1+y_i(X\omega)) + \lambda\Sigma^p_{j=1}|\omega_j|
\end{equation}
\begin{equation}
    \mathcal{L}(\omega) = \Sigma_{i=1}^{n}log(1+y_i(X\omega)) + \lambda\Sigma^p_{j=1}\omega^2_j
\end{equation}

Logistic Regression is designed to be a very efficient algorithm so it computes very fast. In this model, we used grid search to tune the parameter $\lambda$. We used 5-fold cross validation to make our results more reliable.

The best parameter we found using logistic regression is $\lambda = 10$ for both models using $l_1$ and $l_2$ regularizer. Also, we found that within 1000 number of iterations, all the algorithms converges with different $\lambda$. Moreover, models with different $\lambda$ achieved very similar accuracy scores, which indicates that further tuning the regularization strength parameter will not make much improvement. The accuracy score achieved is 85.30\% and the confusion matrix is shown below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{images/logistic_matrix.png}
    \caption{Confusion Matrix of Logistic Regression}
\end{figure}

Based on the confusion matrix, False Positive Rate and False Negative Rate were 20.70\% and 5.09\%, respectively. 

\subsection{K-Nearest Neighbors Classifier}
K-Nearest Neighbors algorithm is a non-parametric classification algorithm that does not need prior training to make prediction due to its own nature. It predicts the "unknown" data point to be the same category as its K nearest neighbors, where distance is determined by a specified distance function. The only parameter to choose for KNN algorithm is the number of neighbors, i.e., "K", which in our case is chosen from 1, 9, 31. Mathematically, KNN algorithm can be expressed as:
\begin{equation}
    \mathcal{P}(\hat{y} = j|X)=\frac{1}{K}\Sigma_{i\in \mathcal{N_0}}I(y_i=j)
\end{equation}
where $\mathcal{N_0}$ is the $K$ nearest neighbors set, and $\hat{y}$ is predicted to be of class $j$ which maximizes the $\mathcal{P}$ above.

KNN algorithm requires computing Euclidean distances between data points, which is typically inefficient. Dimension reduction is often used before applying KNN algorithm because Eculidean distance in high dimension does not contain much information. In our case, however, dimension reduction was not applied to maintain interpretability.

After the best K, whose value is 9, was chosen and applied to the test dataset. The accuracy score achieved by KNN algorithm was 81.3\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/knn_matrix.png}
    \caption{Confusion Matrix of KNN Classification}
\end{figure}

KNN algorithm has its disadvantages besides the one mentioned above, and that is it does not handle categorical data well as Euclidean distance doesn't capture distance between different features. That might explain why accuracy achieved was lower than logistic regression. KNN also has higher False Positive Rate (23.35\%) and higher False Negative Rate (11.58\%). Therefore, KNN is inferior to Logistic regression based on the results.

For more information about KNN algorithm, one can refer to Chapter 2 in the book, \textit{An Introduction to Statistical Learning}\cite{c1}.

\subsection{Random Forest Classifier}
As briefly discussed in the lecture, Random Forest algorithm fits multiple decision trees and to cast the majority vote as the prediction. Bootstrap is embedded in the algorithm to resample data with replacement, which reduces variance of the output but still gives unbiased estimator. The model randomly chooses a subset of features to fit decision tree models to make sure no feature dominates others. By rule of thumb, number of features to randomly choose from the original set of features should be square root of number of features in the original set\cite{c3}.

Three hyperparameters needed to be chosen. Criterion, which is similar to loss function by definition, can be chosen to be Gini-indnex or Cross-entropy; number of decision trees to model was chosen from 10, 50, 100, 300 and 500; the max depth of every tree was chosen from 10, 20 and 50. Grid search was implemented with 5-fold cross-validation to choose hyperparameters. Gini-index and Cross-entropy are defined as below:

\begin{equation}
    G = 1-\Sigma_{i=1}^K(\hat{p_i})^2
\end{equation}

\begin{equation}
    E = -\Sigma_{i=1}^K\hat{p_i}log\hat{p_i}
\end{equation}

where $K=2$ in our case since its binary classification, $\hat{p_i}$ represents the probability of that point being in class $i$.

The best set of hyperparameters indicated that Cross-entropy should be used as criterion, 300 decision trees should be modeled, and max depth of the trees should be 20. With this set of hyperparameters, the model achieved 85.8\% accuracy score based on test data.

For a random decision tree in the decision forest, we extracted one portion of it to visualize (Fig 8):

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{images/rf_visual.png}
    \caption{Visualization of a decision tree in random forest}
\end{figure*}


The resulting confusion matrix is given below:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{images/rf_matrix.png}
    \caption{Confusion Matrix of Random Forest Classification}
\end{figure}
One major advantage of random forest algorithm is that it automatically ranks importance of each feature, which is often used to select features and reduce dimensionality of the data.
The rank of importance is given below:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{images/importance.png}
    \caption{Top 10 important features}
\end{figure}
One can see that Loan purchase is the most important feature identified by random forest algorithm. As one of the objectives of this project, fairness of the model requires that model should not take protected features into consideration, but applicants' race is the 10-th most important feature according to random forest model, more discussion about fairness of the model can be found in next section.


\subsection{Support Vector Machine}
Finally, we fitted Support Vector Machine (SVM) algorithm to the dataset. As discussed in class, SVM is effective in dealing with high dimensional data, and it is also more comfortable with categorical variables than KNN algorithm. Moreover, kernel function for SVM can be versatile or even customized. In our case, radial basis function was used as the kernel function as it is a popular choice for kernel function of SVM classifier. Radial basis function is defined as below:
\begin{equation}
    K(x^{(i)}, x^{(j)}) = exp(-\gamma||x^{(i)}-x^{(j)}||^2), \gamma > 0
\end{equation}
where $\gamma$ is the parameter sets the spread of the kernel. This is not mentioned in class, for more information about kernel function choice, one can refer to book: \textit{An Introduction to Statistical Learning with Applications in R}\cite{c4}.

Then SVM learns a linear classifier:
\begin{equation}
    f(x) = \Sigma_i^N\alpha_iK(x, x^{(i)})
\end{equation}
by solving optimization problem with respect to $\alpha_i$.


Since RBF finds SVM in an infinite dimension, it is impossible to visualize the result. L2 regularization was used because it produces more accurate results than other regularizers. Similar as before, regularization strength was determined by coefficient "C"; $\gamma$ which is the constant coefficient in RBF, scales the influence and directly affects the result, was also chosen through cross-validation.

Cross-validation showed that lower strength of regularization and small scale influence were better for prediction. Feeding the fitted model with test data, accuracy score was 85.57\%. False Positive Rate was 20.25\%, and False Negative Rate was 5.28\%.

The confusion matrix is given below:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{images/svm_matrix.png}
    \caption{Confusion Matrix of SVM Classification}
\end{figure}

\section{Result \& Fairness Analysis}

\subsection{Result Analysis \& WMD}
\begin{table}[h]
\centering
\caption{Classification Results}
\begin{tabular}{|c|c|}
\hline
Classifier & Scores \\\hline
Logistic Regression & 85.3\%\\\hline
KNN & 81.3\%\\\hline
Random Forest & 85.8\%\\\hline
SVM & 85.6\%\\\hline
\end{tabular}
\end{table}
Overall, best accuracy was achieved by Random Forest algorithm with 85.8\% accuracy score. This is reasonable since Random Forest is a very complex nonlinear model. SVM and Logistic Regression also achieved very similar accuracy scores, which indicates that this may be the boundary of this classification problem. If we would like to further improve the performance, more data is necessary in order to get more information.

However, we can see from the results that all of our models have a relatively high False Positive Rates. In the real life, False Positive means we issue mortgage to someone who has a high default probability. This can be very costly for the financial institutions. One possible way to solve the problem could be customizing loss functions which penalizes false positive predictions more.  

We are fairly confident that our project does not produce a Weapon of Math Destruction. We have split the training and testing data so the outcome of our model can be easily tested. Also, although our model still made some wrong predictions which might have negative consequences on the real life, there is still much room for improvement since we have a lot of data (only a very small subset of the HMDA data are used in our project) available. Finally, our model does not create self-fulfilling (or defeating) feedback loops since the prediction results do not change people's behavior of applying mortgages.
\subsection{Fairness Analysis}
Another purpose of this project is to analyze the fairness of our fitted models. Fairness is important because our algorithms could have a big impact if we applied it in the real life. As we mentioned above, false positive predictions can do harm to the financial institutions. One the other hand, false negative predictions also limit the opportunities of getting a mortgage for those who are actually qualified to get. In this project, we focus on analyzing whether race plays an important role on the mortgage application procedure. That is to say, we set the "Race" to be our protected attribute on which discrimination is prohibited.
In order to analyze the fairness, we split our training and testing set according to the applicants' race. Then we ran our fitted models separately on different race groups and calculated the fairness metric. Below are the results and a brief analysis.
\begin{itemize}
\item \textbf{Fairness Metric: Demographic Parity} 

\begin{table}[h]
\centering
\caption{Fairness Metric: Demographic Parity}
\begin{tabular}{|c|c|c|}
\hline
Classifier & White Approval Rate & Black Approval Rate \\\hline
Logistic Regression & 51.0\% & 24.6\%\\\hline
KNN &  53.5\% & 22.2\%\\\hline
Random Forest & 51.9\% & 24.8\%\\\hline
SVM &  50.1\% & 24.6\%\\\hline
\end{tabular}
\end{table}
We first checked whether our algorithms satisfied the demographic parity. By definition, an algorithms satisfies the demographic parity if the prediction is independent of the protected attribute:

\begin{equation}
    \mathrm{P} \hat{y}|a=1 \ \ =\ \ \mathrm{P}\hat{y}|a=0\ \ =\ \ \mathrm{P
    }\hat{y}
\end{equation}

In our case, we would like to check whether our predicted result of mortgage application is independent of an applicant's race. And from TABLE 2 we can see that for all four trained models, the white applicants have an approval rate around $50\%$ while the black applicants have an approval rate around $25\%$, which indicates that white applicants are almost twice likely to get their application approved than black applicants. This result have shown that our prediction is not independent of applicants' race. In other words, our algorithms could be "unfair". However, as we have discussed in the lecture, demographic parity is not a perfect indicator of fairness because it does not take the base rate in different groups into consideration. It could be the case that the black people living in the Columbia Area who applied mortgage have a high default probability in general. Since, we could not observe the base rate directly, it is hard to tell whether the difference between the two groups reflects a sign of discrimination.



\item \textbf{Fairness Metric: Equalized Odds} 

\begin{table}[h]
\centering
\caption{Fairness Metric: Equalized Odds}
\begin{tabular}{|c|c|c|c|c|}
\hline
Classifier & White TPR & Black TPR & White FPR & Black FPR \\\hline
Logistic Reg & 91.9\% & 99.1\% & 27.1\% & 14.1\%\\\hline
KNN &  86.7\% & 92.4\% & 30.3\% & 18.8\% \\\hline
Random Forest & 91.9\% & 99.4\% & 26.5\% & 14.1\%\\\hline
SVM  & 93.0\% & 99.7\% & 27.8\% & 14.1\%\\\hline
\end{tabular}
\end{table}

As an alternative way of ensuring fairness, we then checked whether our models satisfied equalized odds. An algorithm is said to satisfy equalized odds if the predication $\hat{y}$ is independent of the protected attribute $a$ conditional on the outcome $y$:

\begin{equation}
    \hat{y}\ \bot \  a|y \ \ \  \Longleftrightarrow P(\hat{y}|y,a=1)=P(\hat{y}|y,a=0)
\end{equation}

In other words, the true positive, true negative, false positive,
and false negative rates should be the same for both groups. We calculated the True Positive Rate (TPR) and False Positive Rate (FPR) for both groups. The other two rates can be easily inferred from the table. Note that, if the TPR are the same for both groups, then a weak condition of equalized odds: equality of opportunity holds. The results are quite surprising. We found that the black group actually has a higher TPR than the white group. For the Logistic Regression, SVM and Random Forest model, the black group has a TPR even higher than $99\%$. This indicates that for those whose applications should be approved ($y = 1$), our models are in favor of the black group. After looking at the race distribution in the testing data, we thought this could be due to the fact that there are much more observations of the white group. Therefore, our models might have a large probability of making wrong predictions for the white group.

Another interesting results came from the FPR. We found that for all the four fitted models, the white group has a much larger FPR, almost twice, than the black group. This indicates that for those whose application should not be approved ($y=0$), white people are more likely to get their mortgage granted. This means that our models to some extent are in favor of the white group. But it is still hard to say whether the black group is discriminated.


\end{itemize}

\section{Conclusion and Future Works}
In conclusion, we applied four machine learning algorithms, namely logistic regression, KNN, random forest and SVM to classify the mortgage application given each individual applicant's information. After tuning the parameters, our best model random forest achieves an accuracy score of $85.8\%$. 

Also, we checked whether our models are "fair" in term of the applicants' race. We set the applicants' race as the protected attribute and calculated the fairness metrics. Results show that for demographic parity and the FPR, the black group might be discriminated against. But the TPR for the black group is higher than the white group. Further investigation of the base rate for each groups is needed to draw other more reliable conclusions. 


We are fairly confident that our model can be easily generalized well to new data and applied in the real life with some further improvement. Here are some of the possible ways for improvement:
Firstly, we noticed that for our best model random forest, adding further regularization terms or increasing the complexity of the model made little improvement, which indicates that this could the boundary of the problem given the existing data. We might want to include more data points in the training set in order to improve the performance because algorithms can always learn more information with larger data set.

Secondly, we found that our model performed relatively worse in term of the False Positive Rate. As we have discussed above, for the mortgage application, false positive is more costly for the financial institutions in the real life since it means that we issued a mortgage to someone with a high default risk. Hence, we might want to customize the loss functions in order to penalize false positive predictions more.

Lastly, the fairness of our models is still hard to tell. This is partly due to the fact that we could not observe the base rates for different groups directly. Therefore, further investigation of the data is needed. If more evidence of the unfairness of our models can be shown, we should certainly adjust them to address the fairness problem. But there could be a trade-off between accuracy and fairness as we discussed in the lecture.



% \section{Dataset}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.4\textwidth]{frequency}
%     \caption{This graph shows the frequency of each grades, most of loans are graded around A - C.}
%     \label{fig:mesh2}
% \end{figure}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.4\textwidth]{outlier}
%     \caption{This graph shows the annual income of borrowers. The annual income of most borrowers arranges from 0 - 500,000 USD, but some borrowers' annual incomes are higher than 1,000,000 USD and even reach to 7,000,000, which are very doubtful.}
%     \label{fig:mesh2}
% \end{figure}

% \begin{table}[h]
% \centering
% \caption{Data summary}
% \label{table_example}

% \begin{tabular}{|c|c|c|}
% \hline
% Samples & No. of observations & Percentage \\\hline
% Training & 18,921 & 70\%\\\hline
% Testing & 8,109 & 30\%\\\hline
% Total & 27,030 & 100\%\\\hline
% \end{tabular}


% \end{table}






% \subsection{Data and Features}

% The dataset for this project was downloaded directly from the Lending Club Official Website\footnote{https://www.lendingclub.com/info/download-data.action}. Lending Club is one of the leading P2P lending platforms in the US. The original dataset contains 817,163 observations. Considering the limited computational capacity of our own laptops, we randomly dropped 29/30 of the observations and extracted a small sample which has 27,030 observations (Table I). Each observation indicates an individual P2P loan assigned with letter grade standard: "A, B, C, D, E, F, G". Each grade has 5 subgrades arrange from 1-5 (e.g. A1, A2, A3, A4, A5). Most of loans are graded between A - C (approximately 73\% in Figure 1). This dataset contains most of the information provided by borrowers (home\_ownership, annual\-inc, purpose, etc.) and features of the loan (term, status, etc.). When doing the preprocessing work, we found some information provided by the borrowers might not reflect their actual financial condition (Figure 2). This indicates that we may want to remove some outliers in order to increase the accuracy when implementing machine learning algorithms.







% \subsection{Data Preprocessing}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{overview}
%     \caption{This graph shows the distribution of different attributes that we are interested in and their relationships. Here we observed that due to the large number of observations, the points of different classes seem to gather together. However, we can still see there are certain boundaries existing between classes.}
%     \label{fig:mesh1}
% \end{figure}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{hotplot}
% %     \caption{This graph shows the correlation between some important features. We observe strong positive correlation among those bank-credit-related features and a relatively strong negative correlation between annual income and delinquency rate, which are reasonable. Correlation among features could increase the variance of our model.}
%     \label{fig:mesh2}
% \end{figure}

% We did several preprocessing work. Firstly, we handled missing values by filling them with the next valid value. Also, according to our data visualization work, we knew there are extreme outliers existing for some features. We removed observations whose annual income is greater than 30,000. For features like dti, open\_acc, pub\_rec, tot\_cur\_bal, we dropped observation which are 3 standard deviations above the mean. Then we created dummy variables for all the categorical features (term, home\_ownership, verification\_status, etc.). Now, our dataset has 148 columns. To reduce the complexity of the model, we dropped several features which are: categorical features which have been replaced by dummy variables; features that have been shown that do not have great predictive power (state, zipcode); and features that have strong correlations with other features based on the heat map (Figure 4). Some important features can be seen in (Table IV).  After all the preprocessing work, we got a dataset with 67 columns. Figure 3 shows the overall distributions of some features that we cared about. We then converted the letter grades (A - G) into integer values (1 - 7) using LabelEncoder. To train the algorithm, we assigned 70\% of the observations to the training set and the remaining to the testing set. 

% \section{Research Methodology}

% We then implemented the machine learning algorithms to classify the P2P loans. There are many classification models in sklearn for us to choose from. In this project, we used the following models and then compared the results. Note that all the models were implemented through cross\-validation. Moreover, we used Grid Search for all the algorithms in order to tune the parameters.

% The models we used are:

% \begin{itemize}

% \item Logistic Regression
% \item K Nearest Neighbors
% \item Random Forest
% \item Decision Tree
% \item Neural Network using MLPClassifier
% \item Support Vector Machine
% \item Naive Bayes
% \item Neural Network using Keras

% \end{itemize}

% \subsection{Abbreviations and Acronyms} We will use the following abbreviations and acronyms: KNN refers to K Nearest Neighbors. SVM refers to Support Vector Machine.

% \subsection{Baseline Accuracy Score}
% \begin{table}[h]
% \centering
% \caption{Classes Distribution}
% \begin{tabular}{|c|c|}
% \hline
% Class & Distribution \\\hline
% A & 16.40\%\\\hline
% B & 28.06\%\\\hline
% C & 28.74\%\\\hline
% D & 16.11\%\\\hline
% E & 7.88\%\\\hline
% F & 2.49\%\\\hline
% F & 0.59\%\\\hline
% \end{tabular}
% \end{table}

% In our project, we have 7 classes. A simple way to calculate the baseline is to just calculate  $\frac{1}{7}$ = 0.143. This means that if we randomly guess classes, we should get an accuracy score of 14.3\% at least. However, this method ignores the original distribution of the classes in the data. A very important point when doing classification is to check the distribution of different classes in the dataset. Unbalanced class distribution could make the accuracy scores misleading. The distribution of different classes is shown in (TABLE II). We can see that the largest class is "C", which consists of 28.47\% of the data. Therefore, the baseline accuracy score for our model should be 28.47\%. If we have a higher accuracy score than this, our model is doing "something".


% \subsection{Classification Algorithms and Performance Measures}
% Before talking about the algorithms, it is important to discuss how we will evaluate the performance. Generally, accuracy score itself is not a good measure due to the problem of unbalanced classes. A traditional way of evaluating model performances is to look at the confusion matrix. Hence, we plotted the confusion matrices and print the classification reports for all the models we implemented. Accuracy score of individual models will also be used to compare with the baseline accuracy score.

% \begin{itemize}

% \item \textbf{Logistic Regression:} Logistic Regression is one of the fundamental classification methods. It is often used for binary classifications since Sigmoid function gives probabilities of the two classes between 0 and 1. For multiclass predictions, this model works in the a one-vs-all way: it trains multiple logistic regression classifiers, one for each of the K classes in the training dataset. Logistic Regression is designed to be a very efficient algorithm so it computes very fast. In this model, we tuned the parameter "C", which is the inverse of regularization strength.

% \item \textbf{KNN:} KNN algorithm basically classifies data by finding the K nearest data points to it measured by a distance function. Then it assigns the point to the class which is the most common among its K nearest neighbors. KNN works on similarity measures. The accuracy of the model depends greatly on the data itself (KNN is very sensitive to the noises). Finding a good "K" is not very easy and large N requires a lot of calculations. We tested the model using 4 different Ks: 9,15,21,29.

% \item \textbf{Random Forest:} Random Forest is a more advanced ensemble model. It is a bagging model of multiple decision trees. In general, good accuracy can be achieved by this model. But it also involves a lot of calculations and Random Forest can easily result in overfitting due to its high complexity. Also, tuning the parameters of Random Forest is not very easy (choosing a good n\_estimators). Here we tuned the parameters by changing the search range of n\_estimators. We also changed the max\_depth in order to deal with the problem of overfitting. In general, having large number of trees will increase the accuracy of the model, but it will take more time to compute. Therefore, we will not keep testing models with more number of trees.

% \item \textbf{Decision Tree:} In order to compare with Random Forest, we also tested the Decision Tree model. Decision Tree is, in general, a good model if the input has a lot of categorical features. But tuning the parameters of the decision tree is very difficult (when to cut nodes etc.). Since Decision Tree is a "simplified" model of Random Forest, we could expect that its score should be lower than Random Forest.

% \item \textbf{Neural Network using MLPClassifier:} Due to the limited time we had, we tried MLPClassifier from sklearn first. We used the grid search to find the best hidden\_layer\_sizes and best activation function. We expect Neural Network should be a good model due to its complexity. But again, finding a good number of layers and a good number of neurons is very difficult. Here we tested 9 combinations of numbers of hidden layers and neurons.

% \item \textbf{SVM:} SVM constructs a hyperplane or a set of hyperplanes which separate the nearest data points of different classes by the largest distance. SVM is a very powerful model when the classes are gathered together, which can not be dealt with well by Logistic Regression. Disadvantages of SVM are that it also computes very slowly and a good C is often hard to get.

% \item \textbf{Naive Bayes:} We also tried Naive Bayes Model. NB makes very strong assumption that the presence of a particular feature in a class is unrelated to the presence of any other features. However, in the real world, features are usually correlated with each other. Therefore, NB may not perform well in our problem. A good thing about NB is that it computes very fast.

% \item \textbf{Neural Network using Keras:} We also tested Neural Network using Keras. Compared to MLPClassifier, Keras is more convenient to tune parameters.

% \end{itemize}

% \section{Results Analysis}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{matrics}
%     \caption{There shows the confusion matrices that we tested.}
%     \label{fig:mesh2}
% \end{figure}

% \begin{table}[h]
% \centering
% \caption{Classification Results}
% \begin{tabular}{|c|c|}
% \hline
% Classifier & Scores \\\hline
% Logistic Regression & 45.4\%\\\hline
% KNN & 36.0\%\\\hline
% Random Forest & 44.1\%\\\hline
% Decision Tree & 33.7\%\\\hline
% SVM & 44.4\%\\\hline
% Naive Bayes & 37.0\%\\\hline
% Neural Network using SLPClassifier &46.9\%\\\hline
% Neural Network using Keras & 46.6\%\\\hline
% \end{tabular}
% \end{table}

% Now we have implemented all the above algorithms and the accuracy scores of each model have been shown in (TABLE III). Confusion Matrices have also been displayed in (Fig 5). Let's analyze each of the model according to the accuracy scores. Note that all the models were implemented using cross\-validation.

% \begin{itemize}

% \item \textbf{Logistic Regression:} Logistic Regression showed a cross\-validation accuracy score of 45.4\%, which is much higher than the baseline (28.47\%). Grid Search found the best C is 0.91. The good performance may result from the fact that Logistic Regression is very easy to tune parameters. Also, sometimes a simple model just fit the data well.

% \item \textbf{KNN:} KNN got a cross\-validation accuracy score of 36.0\%. The grid search gave us the best K which is 29. Increasing K gave us better results. However, it also increases the time of computation. KNN does not work very well here. We mentioned above that KNN is very sensitive to noises of the data. Finding a good K is also not easy. Due to the limited time of this project, we will not keep trying different Ks.

% \item \textbf{Random Forest:} Random Forest achieved very close score (44.1\% accuracy) to Logistic Regression. Grid search found the best parameters are 700 trees. We limited the maximum depth to be 20 in order to avoid overfitting. From the cross\-validation result we observed that with more trees, the results will become better. We can also get better results if we allow the maximum depth to become larger. But all of these will result in longer computation time. We will not keep trying larger number of trees due to the time constraint.

% \item \textbf{Decision Tree:} As expected, Decision Tree performed worse than Random Forest (Random Forest is actually a bagging model of Decision Tree). It got a cross\-validation accuracy score of 33.7\%. This might also result from the difficulty of tuning parameters of this model (when to cut nodes, how deep should the tree go.)

% \item \textbf{Neural Network using MLPClassifier:} Neural Network using MLPClassifier achieved the best cross\-validation accuracy score of 46.9\%. This is reasonable since Neural Network is a very complex model. Grid search chose the best model to have two hidden layers with 10 neurons in each layer.

% \item \textbf{SVM:} Again, through tuning parameters, we achieved good accuracy scores (44.4\%) by SVM, which is very close to Random Forest and Logistic Regression. We have not tested different gamma for rbf function (it took a very long time to run SVM algorithms). Better scores can be achieved by tuning parameters more carefully.

% \item \textbf{Naive Bayes:} As expected, Naive Bayes did not perform well. It only got a cross-validation accuracy score of 37.0\%. This is because Naive Bayes makes a very strong assumption which might not be held in the reality.

% \item \textbf{Neural Network using Keras:} We also tested Neural Network using Keras. Through tuning the number of hidden layers and number of neurons, we achieved a good accuracy score of 46.6\%. This model is not overfitting a lot and the graph of training and validation loss is in good shape.

% \end{itemize}

% \section{CONCLUSIONS AND FUTURE WORKS}

% We can see that all the models have achieved higher accuracy scores than both randomly guessing (14.29\%) and guessing the largest class (28.47\%). That is to say, our models are doing something. However, even the best model only got a cross\-validation accuracy score around 47\%. This implies that although our model is three times better than randomly guessing, it still only classified half of the P2P loans correctly. This can be costly in real life. 

% Also, as the confusion matrices indicated, our model does not have a great predictive power over loans which were rated in lower grades like "F" or "G". This is because we did not have a lot of training points in these classes. However, in real life, we do care about these "bad" loans because lower grades often indicate a higher probability of default and therefore indicates higher credit risk.

% Hence, there is much space for improvement. Here are some of them:

% First of all, we only used a small portion of the data due to the limit of our laptops computational capacity. With the help of HPC, we can test the models using the whole dataset, which may give us very different results.

% Secondly, due to the limited time, we did not test the models using a lot of different parameters. Algorithms like KNN, Decision Tree, SVM are sensitive to the parameters and we may get better results if more parameters tuning work could be done.

% Thirdly, we may want to test the stacking model in more details. We can apply different meta classifier functions or test different combinations of first-level classifiers in order to achieve better results.

% Lastly, we may want to balance the class distribution in the training data in order to make our model have more predictive power over loans with lower grades. This can be done by manually adding more observations with lower classes into the training data.

% However, we also need to admit that there is certain limit existing about the best performance of machine learning algorithms on a specific classification problem. In some cases, the data itself does not have a very nice distribution and all the classes are just gathered together, which can result in great difficulty classifying them well.

% In conclusion, our project introduced a machine learning approach to classify P2P loans into 7 categories. We tested different models and analyzed the results in detail. Our best model, in general, can classify P2P loans correctly into one of the seven classes with a cross\-validation accuracy score around 47\%. There are a lot of improvements can be done in the future.




% \section*{APPENDIX}

% \begin{table}[h]

% \caption{Some important variables we used to build the model}


% \begin{tabular}{|C|c|L|}
% \hline
% Variables & Type & Description\\\hline
% amount (funded\_amnt) & Scale & the amount of the loan\\\hline
% term & Scale & the term of the loan\\\hline
% emp\_length & Scale & Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. \\\hline
% home\_ownership & Categorical & the house ownership of the borrower\\\hline
% annual\_inc & Scale &annual income of the borrower (claimed by the borrower)\\\hline
% verification\_status & Categorical & whether the source of the annual income claimed by the borrower was verified\\\hline
% loan\_status & Categorical &  the current status of the loan\\\hline
% purpose & Categorical & the purpose for the borrower to borrow the loan\\\hline
% dti & Scale & A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income\\\hline
% delinq\_2yrs & Scale & The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years\\\hline
% inq\_last\_6mths & Scale & The number of inquiries in past 6 months (excluding auto and mortgage inquiries)\\\hline
% open\_acc & Scale & The number of open credit lines in the borrower's credit file\\\hline

% pub\_rec & Scale & Number of derogatory public records\\\hline

% revol\_util & Scale & Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\\\hline
% tot\_cur\_bal  & Scale & Total current balance of all accounts\\\hline
% bc\_util & Scale & Ratio of total current balance to high credit/credit limit for all bankcard accounts.\\\hline







% \end{tabular}
% \end{table}

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{APPENDIX}

%Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The author would like to thank Dr.Enric Junque de Fortuny, Dr.Xianbin Gu, Ruowen Tan for their generous help.



\begin{thebibliography}{99}

\bibitem{c1} G. James, D. Witten, T. Hastie and R. Tibshirani. "K-Nearest Neighbors," in \textit{An Introduction to Statistical Learning with Applications in R}, 7th ed. New York, NY: Springer, 2015, pp. 39-42.

\bibitem{c2} N. Chawla, K. Bowyer, L. Hall and W. Kegelmeyer. \textit{SMOTE: Synthetic Minority Over-sampling Technique}. Journal of Artificial Intelligence Research 16, 2002, pp. 321–357. 
\bibitem{c3} G. James, D. Witten, T. Hastie and R. Tibshirani. "Random Forests," in \textit{An Introduction to Statistical Learning with Applications in R}, 7th ed. New York, NY: Springer, 2015, pp. 319-321.
\bibitem{c4} G. James, D. Witten, T. Hastie and R. Tibshirani. "Support Vector Machines," in \textit{An Introduction to Statistical Learning with Applications in R}, 7th ed. New York, NY: Springer, 2015, pp. 319-321.

% \bibitem{c3} Based on the British Peer-to-Peer Finance Association’s definition of peer-to-peer lending. 

% \bibitem{c4} Block, J., \& Sandner, P. (2009). What is the effect of the financial crisis on venture capital financing? Empirical evidence from US Internet start-ups. Venture Capital–an International Journal of Entrepreneurial Finance, 11(4), 295–309. 

% \bibitem{c5}Bruton, G., Khavul, S., Siegel, D., \& Wright, M. (2015). New financial alternatives in seeding entrepreneurship: Microfinance, crowdfunding, and peer-to-peer innovations. Entrepreneurship: Theory and Practice, 39, 9–26.

% \bibitem{c6} Chris Barth, Lend Thy Neighbor,FORBES, June 6, 2012, at 172.

% \bibitem{c7}Hsu, Sara, Jianjun Li and Yanzhi Qin (2013): “Shadow Banking and Systemic Risk in Europe and China”, City Political Economy Research.

% \bibitem{c8} Jagtiani, J., \& Lemieux, C.e (2018). The roles of alternative data and machine learning in fintech lending: Evidence from the lendingClub consumer platform. Federal Reserve
% Bank of Philadelphia Working Paper No. 18-15 Available at https://www.philadelphiafed.org/-/media/research-and-data/publications/working-papers/2018/
% wp18-15.pdf.

% \bibitem{c9} Jianjun Li, Sara Hsu, Zhang Chen \& Yang Chen (2016) Risks of P2P Lending Platforms in China: Modeling Failure Using a Cox Hazard Model, The Chinese Economy, 49:3, 161-172

% \bibitem{c10} Judge, Kathryn, The Future of Direct Finance: The Diverging Paths of Peer-to-Peer Lending and Kickstarter (September 18, 2015). 50 Wake Forest Law Review, Forthcoming; Columbia Law and Economics Working Paper No. 520. Available at SSRN: https://ssrn.com/abstract=2662697 or http://dx.doi.org/10.2139/ssrn.2662697
% Centre, Working Paper Series No 2013/02.

% \bibitem{c11} K. Branker , E. Shackles \& J. M. Pearce (2011) Peer\-to\-peer financing mechanisms to accelerate renewable energy deployment, Journal of Sustainable Finance & Investment, 1:2, 138-155.

% \bibitem{c12} Lu Y, Gu B, Ye Q, Sheng Z. 2012. Social influence and defaults in peer-to-peer lending networks. Presented at Int.
% Conf. Inf. Syst., Orlando, FL.

% \bibitem{c13} Mach, Traci and Carter, Courtney and Slattery, Cailin, Peer-to-Peer Lending to Small Businesses (January 9, 2014). FEDS Working Paper No. 2014-10. Available at SSRN: https://ssrn.com/abstract=2390886 or http://dx.doi.org/10.2139/ssrn.2390886.

% \bibitem{c14} Mezei J, Byanjankar A, Heikkilä M. Credit risk evaluation in peer-to-peer lending with linguistic data transformation and supervised learning[J]. 2018.

% \bibitem{c15} Moenninghoff, Sebastian Christoph and Wieandt, Axel, The Future of Peer-to-Peer Finance (May 20, 2012). Zeitschrift für Betriebswirtschaftliche Forschung, August/September 2013, p. 466-487 . Available at SSRN: https://ssrn.com/abstract=2439088.

% \bibitem{c16} Moore \& Alloway, supra note 87.

% \bibitem{c17} Namvar, Ethan, An Introduction to Peer-to-Peer Loans as Investments (September 6, 2013). Journal of Investment Management First Quarter, 2014. Available at SSRN: https://ssrn.com/abstract=2227181 or http://dx.doi.org/10.2139/ssrn.2227181.

% \bibitem{c18} Riza Emekter, Yanbin Tu, Benjamas Jirasakuldech & Min Lu (2015) Evaluating credit risk and loan performance in online Peer-to-Peer (P2P) lending, Applied Economics, 47:1, 54-70, DOI: 10.1080/00036846.2014.962222.

% \bibitem{c19} S. Paul, “Creditworthiness of a Borrower and the Selection Process in Micro-finance: A Case Study from the Urban Slums of India,” J. Appl. Econ. Res., vol. 8(1), pp. 59-75, 2014.

% \bibitem{c20} Sundararajan, A. (2014). Peer-to-peer businesses and the sharing (collaborative) economy: Overview, economic effects and regulatory issues. Written testimony for the hearing titled the power of connection: Peer to peer businesses, January. Retrieved from http://smallbusiness.house.gov/uploadedfiles/1-15-2014\_ revised\_sundararajan\_testimony.pdf.

% \bibitem{c21} Wei, Shen, Shadow Banking System in China – Origin, Uniqueness and Governmental Responses (September 4, 2013). Journal of International Banking Law and Regulation, Issue 1, (2013). Available at SSRN: https://ssrn.com/abstract=2320407.







\end{thebibliography}




\end{document}
